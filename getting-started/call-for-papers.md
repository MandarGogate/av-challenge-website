## Call for papers: 3rd COG-MHEAR Audio-Visual Speech Enhancement Challenge (AVSEC-3) 

The COG-MHEAR Audio-Visual Speech Enhancement Challenge (AVSEC) sets the first benchmark in the field, providing a carefully designed dataset and scalable protocol for human listening evaluation of audio-visual speech enhancement systems. The open AVSEC framework aims to stimulate collaborative research and innovation to enable the development and evaluation of next-generation audio-visual speech enhancement and separation systems, including multimodal assistive hearing and communication technologies.
Two previous successful editions of the Challenge (organised as part of IEEE SLT 2022 and IEEE ASRU 2023) show a general trend of system improvement but indicate that an intelligibility gap remains when compared to clean speech.
We are running a third edition of the Challenge, AVSEC-3, as a satellite Workshop of INTERSPEECH 2024, to be held on 1 September, in Kos Island, Greece.


The AVSEC results will be announced at the workshop. Furthermore, the workshop will showcase ongoing technology developments in audio-visual speech enhancement, and provide networking and collaborative space for participants to reflect on the scope and limitations of current speech and hearing technologies.


Accepted Workshop papers (both short 2-page and full-length papers of 4-6 pages following the INTERSPEECH 2024 paper template) will be published in ISCA Proceedings.


Authors of selected papers (including winners and runner-ups of each Challenge Track) will be invited to submit significantly extended papers for consideration in a Special Issue of the [IEEE Journal of Selected Topics in Signal Processing (JSTSP)](https://signalprocessingsociety.org/publications-resources/special-issue-deadlines/ieee-jstsp-special-issue-deep-multimodal-speech-enhancement-and-separation)

## Challenge Registration

To register for the challenge please follow the guidelines on the website:
https://challenge.cogmhear.org/#/getting-started/register

## Paper Submission and Topics

We invite prospective authors to submit either 2-page short paper or full-length papers of 4-6 pages following the INTERSPEECH 2024 paper template.

We welcome submissions from participants of the second (AVSEC-2) and third editions (AVSEC-3) of the Challenge, and also invite submissions on related research topics, including but not limited to the following:

- Low-latency approaches to audio-visual speech enhancement and separation.
- Human auditory-inspired models of multi-modal speech perception and enhancement.
- Energy-efficient audio-visual speech enhancement and separation methods.
- Machine learning for diverse target listeners and diverse listening scenarios.
- Audio quality & intelligibility assessment of audio-visual speech enhancement systems.
- Objective metrics to predict quality & intelligibility from audio-visual stimuli.
- Understanding human speech perception in competing speaker scenarios.
- Clinical applications of audio-visual speech enhancement and separation, e.g. multi-modal hearing assistive technologies for hearing-impaired listeners, and speech-enabled communication aids to support  autistic people with speech disorders.
- Accessibility and human-centric factors in the design and evaluation of innovative multimodal technologies, including multi-modal corpus development, public perceptions, ethics considerations, standards, societal, economic and political impacts. 


[//]: # (Important dates can be found [here.]&#40;https://challenge.cogmhear.org/#/important-dates&#41;)

## Important dates:

- 16th February 2024: Release of training and development data
- 16th February 2024: Release of baseline system
- 10th April 2024: Evaluation data release
- 10th April 2024: Leaderboard open for submissions
- 6th May 2024: Paper submission opens
- 20th June/2024: Deadline for challenge submissions
- 28th June 2024: Paper submission closes
- 12th July: Acceptance notification
- 26th July: early release of evaluation results
- 1st August 2024: camera-ready paper.


## International Scientific Committee
We have a distinguished AVSEC scientific committee to oversee the review process. This includes academics and industry experts from the speech and hearing communities, with a similar profile to that of our previously organised international workshops: Challenges and Opportunities in Developing Multi-Modal, Transformative Hearing Assistive Technologies;  Advances on multi-modal Hearing Assistive Technologies (AMHAT 2023) and Multi-talker methods in speech processing

- Michael Akeroyd (co-Chair), University of Nottingham, UK 
- Yu Tsao (co-Chair), Academic Sinica, Taiwan
- Peter Derleth,, Sonova AG
- John Hansen, University of Texas at Dallas, USA
- Naomi Hart, Trinity College, Ireland
- Shinji Watanabe, Carnegie Mellon University
- Nima Mesgarani, Columbia University, USA
- Jesper Jensen, Aalborg University, Denmark
- Qiang Huang, University of Sunderland, UK
- Bernd T. Meyer, University of Oldenburg, Germany
- James M. Kates, University of Colorado, USA
- Isabel Trancoso, IST, Univ. Lisbon, Portugal
- Volker Hohmann, University of Oldenburg, Germany
- Emanuel Habets, University of Erlangen-Nuremberg, Germany
- Chi-Chun Lee, National Tsing Hua University, Taiwan
- Sharon Gannot, Bar-Ilan University, Israel
- Yong Xu, Tencent America, USA
- Daniel Michelsanti, Aalborg University, Denmark
- Dong Yu, Tencent AI Lab, China
- Marc Delcroix, NTT Communication Science Laboratories, Japan
- Zheng-Hua Tan, Aalborg University, Denmark
- Harish Chandra Dubey, Microsoft, USA
- Simon Doclo, University of Oldenburgh, Germany
- Ben Milner, University of East Anglia, UK
- Hadi Larijani, Glasgow Caledonian University, UK
- Mandar Gogate, Edinburgh Napier University, UK
- Lorena Aldana, University of Edinburgh, UK
- Shixiong Zhang, Tencent AI Lab, USA
- Alex Casson, University of Manchester, UK
- Jennifer Williams, University of Southampton, UK
- Erfan Loweimi, University of Cambridge and Edinburgh Napier University, UK
- Raza Varzandeh, University of Oldenburg, Germany
- Kia Dashtipour, Edinburgh Napier University, UK
- Qammer Abbasi, University of Glasgow, UK
- Adeel Ahsan, University of Stirling, UK
- Mathini Sellathurai, Heriot-Watt University, UK
- Tharm Ratnarajah, University of Edinburgh, UK
- Jen-Cheng Hou, Academia Sinica, Taiwan
- Tughrul Arslan, University of Edinburgh, UK
- Hsin-Min Wang, Academia Sinica, Taiwan
- Muhammad Imran, University of Glasgow, UK
- Cassia Valentini Botinhao, University of Edinburgh, UK
- Jun-Cheng Chen, Academia Sinica, Taiwan
