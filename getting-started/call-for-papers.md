## Call for papers: 4th COG-MHEAR Audio-Visual Speech Enhancement Challenge (AVSEC-4) 

The COG-MHEAR Audio-Visual Speech Enhancement Challenge (AVSEC) sets the first benchmark in the field, providing a carefully designed dataset and scalable protocol for human listening evaluation of audio-visual speech enhancement systems. The open AVSEC framework aims to stimulate collaborative research and innovation to enable the development and evaluation of next-generation audio-visual speech enhancement and separation systems, including multimodal assistive hearing and communication technologies.
Three previous successful editions of the Challenge (organised as part of IEEE SLT 2022, IEEE ASRU 2023 and Interspeech 2024) show a general trend of system improvement but indicate that an intelligibility gap remains when compared to clean speech.

[//]: # (AVSEC-4 will be held as a satellite Workshop. Further information about the workshop will be announced soon. )

We are happy to announce that we are running a fourth edition of the Challenge. AVSEC-4 will be held as a satellite Workshop of [Interspeech 2025](https://www.interspeech2025.org/satellite-events).

Date: 16th of August
Location: Rotterdam, The Netherlands.


Accepted Workshop papers (both short 2-page and full-length papers of 4-6 pages following the [Interspeech 2025 paper template](https://www.interspeech2025.org/author-resourceshttps://www.interspeech2025.org/author-resources)) will be published in ISCA Proceedings.


[//]: # (Authors of selected papers &#40;including winners and runner-ups of each Challenge Track&#41; will be invited to submit significantly extended papers for consideration in a Special Issue of the [IEEE Journal of Selected Topics in Signal Processing &#40;JSTSP&#41;]&#40;https://signalprocessingsociety.org/publications-resources/special-issue-deadlines/ieee-jstsp-special-issue-deep-multimodal-speech-enhancement-and-separation&#41;)

## Challenge Registration

To register for the challenge please follow the guidelines on the website:
https://challenge.cogmhear.org/#/getting-started/register

## Workshop paper submission and topics

Paper submission is open! 
You can make your submission [here.](https://cmt3.research.microsoft.com/AVSEC2025)

*Topics*

We invite prospective authors to submit either 2-page short paper or full-length papers of 4-6 pages.  

We welcome participants of AVSEC-4, previous Challenge editions (AVSEC-2 and AVSEC-3), and also those not participating in AVSEC, to submit papers on related research topics including but not limited to the following:

- Low-latency approaches to audio-visual speech enhancement and separation.
- Human auditory-inspired models of multi-modal speech perception and enhancement.
- Energy-efficient audio-visual speech enhancement and separation methods.
- Machine learning for diverse target listeners and diverse listening scenarios.
- Audio quality & intelligibility assessment of audio-visual speech enhancement systems.
- Objective metrics to predict quality & intelligibility from audio-visual stimuli.
- Understanding human speech perception in competing speaker scenarios.
- Clinical applications of audio-visual speech enhancement and separation, e.g. multi-modal hearing assistive technologies for hearing-impaired listeners, and speech-enabled communication aids to support  autistic people with speech disorders.
- Accessibility and human-centric factors in the design and evaluation of innovative multimodal technologies, including multi-modal corpus development, public perceptions, ethics considerations, standards, societal, economic and political impacts. 


[//]: # (Important dates can be found [here.]&#40;https://challenge.cogmhear.org/#/important-dates&#41;)

## Important dates:

- ~~End of February 2025~~ 21st March: Release of training and development data. 
- ~~End of February 2025~~ 2nd April: Release of baseline systems (binaural and monoaural).
- ~~May 2025~~ 6th June: Evaluation data release. 
- ~~May 2025~~ 9th June: Leaderboard open for submissions. 
- ~~May 2025~~ 12th June: Paper submission opens. 
- 24th June 2025: Additional "out-of-domain" evaluation corpus released.
- ~~27th June 2025~~ (Extended) 7th July 2025:: Deadline for challenge submissions and one-page system description submission.
- ~~6th July 2025~~ (Extended) 11th July 2025:: Workshop paper submission closes.
- 14th July 2025: Early paper acceptance notifications. 
- ~~10th July 2025~~ 23rd July 2025: Early release of evaluation results.
- 1st August 2025: camera-ready paper. 

[//]: # (## International Scientific Committee)

[//]: # (We have a distinguished AVSEC scientific committee to oversee the review process. This includes academics and industry experts from the speech and hearing communities, with a similar profile to that of our previously organised international workshops: Challenges and Opportunities in Developing Multi-Modal, Transformative Hearing Assistive Technologies;  Advances on multi-modal Hearing Assistive Technologies &#40;AMHAT 2023&#41; and Multi-talker methods in speech processing)

[//]: # ()
[//]: # (- Michael Akeroyd &#40;co-Chair&#41;, University of Nottingham, UK )

[//]: # (- Yu Tsao &#40;co-Chair&#41;, Academic Sinica, Taiwan)

[//]: # (- Peter Derleth,, Sonova AG)

[//]: # (- John Hansen, University of Texas at Dallas, USA)

[//]: # (- Naomi Hart, Trinity College, Ireland)

[//]: # (- Shinji Watanabe, Carnegie Mellon University)

[//]: # (- Nima Mesgarani, Columbia University, USA)

[//]: # (- Jesper Jensen, Aalborg University, Denmark)

[//]: # (- Qiang Huang, University of Sunderland, UK)

[//]: # (- Bernd T. Meyer, University of Oldenburg, Germany)

[//]: # (- James M. Kates, University of Colorado, USA)

[//]: # (- Isabel Trancoso, IST, Univ. Lisbon, Portugal)

[//]: # (- Volker Hohmann, University of Oldenburg, Germany)

[//]: # (- Emanuel Habets, University of Erlangen-Nuremberg, Germany)

[//]: # (- Chi-Chun Lee, National Tsing Hua University, Taiwan)

[//]: # (- Sharon Gannot, Bar-Ilan University, Israel)

[//]: # (- Yong Xu, Tencent America, USA)

[//]: # (- Daniel Michelsanti, Aalborg University, Denmark)

[//]: # (- Dong Yu, Tencent AI Lab, China)

[//]: # (- Marc Delcroix, NTT Communication Science Laboratories, Japan)

[//]: # (- Zheng-Hua Tan, Aalborg University, Denmark)

[//]: # (- Harish Chandra Dubey, Microsoft, USA)

[//]: # (- Simon Doclo, University of Oldenburgh, Germany)

[//]: # (- Ben Milner, University of East Anglia, UK)

[//]: # (- Hadi Larijani, Glasgow Caledonian University, UK)

[//]: # (- Mandar Gogate, Edinburgh Napier University, UK)

[//]: # (- Lorena Aldana, University of Edinburgh, UK)

[//]: # (- Shixiong Zhang, Tencent AI Lab, USA)

[//]: # (- Alex Casson, University of Manchester, UK)

[//]: # (- Jennifer Williams, University of Southampton, UK)

[//]: # (- Erfan Loweimi, University of Cambridge and Edinburgh Napier University, UK)

[//]: # (- Raza Varzandeh, University of Oldenburg, Germany)

[//]: # (- Kia Dashtipour, Edinburgh Napier University, UK)

[//]: # (- Qammer Abbasi, University of Glasgow, UK)

[//]: # (- Adeel Ahsan, University of Stirling, UK)

[//]: # (- Mathini Sellathurai, Heriot-Watt University, UK)

[//]: # (- Tharm Ratnarajah, University of Edinburgh, UK)

[//]: # (- Jen-Cheng Hou, Academia Sinica, Taiwan)

[//]: # (- Tughrul Arslan, University of Edinburgh, UK)

[//]: # (- Hsin-Min Wang, Academia Sinica, Taiwan)

[//]: # (- Muhammad Imran, University of Glasgow, UK)

[//]: # (- Cassia Valentini Botinhao, University of Edinburgh, UK)

[//]: # (- Jun-Cheng Chen, Academia Sinica, Taiwan)
