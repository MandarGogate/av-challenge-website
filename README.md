# 1st COG-MHEAR Audio-Visual Speech Enhancement Challenge (AVSE)
## A machine learning challenge for next-generation hearing devices

### Challenge Summary
Human performance in everyday noisy situations is known to be dependent upon both aural and visual senses that are contextually combined by the brain’s multi-level integration strategies. The multimodal nature of speech is well established, with listeners known to unconsciously lip read to improve the intelligibility of speech in a real noisy environment.  It has been shown that the visual aspect of speech has a potentially strong impact on the ability of humans to focus their auditory attention on a particular stimulus.

The aim of the first Audio-Visual Speech Enhancement (AVSE) Challenge is to bring together the wider computer vision, hearing and speech research communities to explore novel approaches to multimodal speech-in-noise processing. Both raw and pre-processed AV datasets – derived from TED talk videos – will be made available to participants for training and development of audio-visual models to perform speech enhancement and speaker separation at SNR levels that will be significantly more challenging than typically used in audio-only scenarios. Baseline models will be provided along with scripts for objective evaluation. Challenge evaluation will utilise established objective measures such as STOI and PESQ as well as subjective intelligibility tests with human subjects.

