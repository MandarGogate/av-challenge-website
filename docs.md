# Overview

<p align="center">
  <img src="AVSE-challenge-diagram.jpg" alt="diagram" width="700"/>
</p>


### Introduction

Human performance in everyday noisy situations is known to be dependent upon both aural and visual senses that are contextually combined by the brain’s multi-level integration strategies. The multimodal nature of speech is well established, with listeners known to unconsciously lip read to improve the intelligibility of speech in a real noisy environment.  It has been shown that the visual aspect of speech has a potentially strong impact on the ability of humans to focus their auditory attention on a particular stimulus.

The aim of the first Audio-Visual Speech Enhancement (AVSE) Challenge is to bring together the wider computer vision, hearing and speech research communities to explore novel approaches to multimodal speech-in-noise processing. Both raw and pre-processed AV datasets – derived from TED talk videos – will be made available to participants for training and development of audio-visual models to perform speech enhancement and speaker separation at SNR levels that will be significantly more challenging than typically used in audio-only scenarios. Baseline models will be provided along with scripts for objective evaluation. Challenge evaluation will utilise established objective measures such as STOI and PESQ as well as subjective intelligibility tests with human subjects.

### Announcements

You can sign up to receive announcements at our mailing list (avse-challenge@mlist.is.ed.ac.uk).

#### Subscribe
Send a message to sympa@mlist.is.ed.ac.uk from the address you want to subscribe to the list.  
In the subject line of your message, type in:  
subscribe avse-challenge YourFirstName YourFamilyName  
Leave the message body blank.

#### Unsubscribe
Send a message to sympa@mlist.is.ed.ac.uk from the address you used to subscribe to the list.  
In the subject line of your message, type in:  
unsubscribe avse-challenge  
Leave the message body blank.

#### 2nd edition of the AVSE Challenge

We are conducting a 2nd edition of the Audio-Visual Speech Enhancement challenge in 2023. We will run the 2nd edition of the Challenge using identical train and dev datasets to those used in the 1st edition.

Information about the submission deadline is available [here](/important-dates.md).

### Challenge Data

**Note that this dataset is identical to that used in the 1st edition of the Challenge, <avse1_data_v2.tar>**

- [Scene Generation](/challenge-data/scene-gen.md) - a description of the listening scenario and how it has been simulated.
- [Data Specifications](/challenge-data/data-spec.md) - the data that can be used to train and evaluate your system during development.

### Software
- [Baseline model](/software/baseline.md) - a fully functioning baseline model is provided to challenge participants
- [Core Software](/software/core.md) - the  tools that we are providing to help you generate more data and evaluate a challenge entry.

### Taking part
- [Registration](/getting-started/register.md) - Form for participant registration
- [Submission and Evaluation](/getting-started/submission.md) - information about how to prepare your submission and the tests which will be used to evaluate the best systems.
- [Rules](/getting-started/rules.md) - the rules to which all challenge entries must adhere.

### Download
- [Baseline model and Data](/download.md) - software and challenge data can be downloaded from here

### Results

TBA

[//]: # (- [Listening test]&#40;/results.md&#41;)
[//]: # (- [Leaderboard]&#40;/leaderboard.md&#41;)
